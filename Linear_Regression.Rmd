---
title: "Linear Regression"
author: "Hussain Alsalman"
date: "1/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("here")
library("ggplot2")
library("scales")
library("gganimate")
library("broom")
library("DAAG")
```

# Linear Regression 

## So what is Linear Regression model?

Linear regression attempts to model the relationship between two variables by fitting a linear equation to observed data. One variable is considered to be an explanatory variable, and the other is considered to be a dependent variable.




Let's walk through an example to see the motivation and the idea behind linear regression.

We can estimate the expected value of `income` by simply calculating the the mean. In this case we did not really bother to utilize what we know about the `age` of the individuals. 


```{r, message=FALSE}

reg_df <- data.frame(age = c(2,2,3,4,4,5,6,7,8,9), income = c(10:15,20,18,22,25))

# We wants to predict Y 
g1 <- ggplot(reg_df, mapping = aes(x = age, y = income)) + 
  geom_point() 

g2<- g1+
  
  # We can estimate the line by only the mean .. and we did not really bother to use the information in the Age. 
  geom_hline(aes(yintercept = mean(income)), linetype = "solid", color = "red") + 
  geom_text(aes(y = mean(income), x=0, label = paste("Mean ", mean(income)) ),color = "red", nudge_x = 2, nudge_y = 0.4) + 
  
  geom_segment(mapping= aes(y = mean(income), yend = income, x = age, xend = age ), linetype = "dashed", color = "navy")

  #geom_smooth(method = 'lm', se = FALSE)
g2
```

We can write the formula as such 

$$\hat{y} = \frac{1}{n}\sum_{i = 1}^{n}{y_{i}} = E[Y] = `r  mean(reg_df$income)` $$

As shown in the graph our estimation is not accurate. This inaccuracy can be measured by the sum of the squared error. sum of the squared difference between the dependent variable and the mean is called **Sum of Squares Total Errors (SST)**. We calculate it using this formula 

$$
SST = \sum_{i=1}^{n}(y_{i} - \bar{y})^2
$$


In this case our $SST$ is `r sum((reg_df$income - mean(reg_df$income))^2) #228`

## Can we do better? 

Of course!! this is where **Linear Regression** come to rescue. Linear Regression is all about fitting the best line that minimize the error. 
Since we are dealing with only one variable, we can achieve that by changing the only slope and the intercept of this line. 

If you remember from high school this is the equation of the line. 

$$ 
y = a + bx + \epsilon
$$ 

let's try to find better line


```{r}
line_df <- data.frame(
  a = round(seq(0,2,length.out = 50),2),
  b = round(seq(16,6, length.out = 50),2),
  time = 1:50

)

 g1 + geom_abline(data = line_df, aes(slope= a, intercept=b )) +  
  transition_components(time=time) + 
  scale_y_continuous(limits = range(0,25))
  

```

We came up with this line. It looks good, don't you think?

notice that we were lucky that some points were exactly on the line. This is not always good though, we might be **overfitting** the model. 

```{r}
 g1 + geom_abline(slope = 2,intercept = 6) + 
  geom_segment(mapping = aes(x=age , xend = age , y =income , yend = 2*age+6 ), color = "orangered2")
 
```



When we calculate the error for this line we get **Sum of Squares Errors (SSE)**  of `r sum(round(lm(income~age, data = reg_df)$residuals)^2) `

$$
SSE = \sum_{i=1}^{n}(y_{i} - \hat{y}_i)^2
$$

# let's compare both lines and see how different they are 

```{r, out.width="%100"}
 g2 + geom_abline(slope = 2,intercept = 6) + 
  geom_segment(mapping = aes(x=age , xend = age , y =income , yend = 2*age+6 ), color = "orangered2")
 
```


Notice at point `age = 6` and `income = 20`. The distance between our line and the **mean** is called the **Regression Error** or **Model Error** or **SSR**. 

When we calculate this error we get 

$$
SSR = \sum_{i=1}^{n}( \hat{y}_i-\bar{y})^2
$$

```{r}
round(anova(lm(income~age, data = reg_df))$`Sum Sq`[1]) 
```

And the relationship between the three Errors are as such 

$$SST = SSE + SSR $$


# Important concepts 

It is all good and awesome to know the sum of squared error. But this is not very useful because it seems that there is a relationship between the sample size and the errors. We would like to see a measure of error without sample size effect.  Your next friend is **MSE** or **Mean of Squared Error**.  And here is the formula 

$$
MSE = \frac{SSE}{n-2} 
$$

We can derive **RMSE** by simply taking the square root from **MSE**

$$
RMSE = \sqrt{MSE} 
$$
 
# So how do we know if our model is good? 

One way to measure is to see how much variance our model has explained in comparison with the **mean** model. 

For the purposes of this course all you need to know that also 

$$ R^2 = \frac{SSR}{SST} $$ 

This is called **Coefficient of Determination**

# R can magically do all this calculation in really simple code. 

```{r, echo = TRUE}
# This is our data
reg_df <- data.frame( age = c(2,2,3,4,4,5,6,7,8,9), income = c(10:15,20,18,22,25) )

m_1 <- lm( income ~ age , data = reg_df)

summary(m_1)

```


### Let's assume now that we have new preson and we know his/her age information and we want to estimate the his/her income 

```{r}

new_df <- data.frame(age=c(5,2,6,9,1,4,4,2), income = c(16,20,14,13,25,18,9,23))

#our estimation by using the predict function 
a <-predict(m_1,newdata = new_df, se.fit = TRUE)

# or we can simply substitute in the function  y = 2 *(5) + 6 
```


# Hands on : Exercise 

```{r}
file_path <- here("assets", "Data", "sat_and_college_gpa.txt")
df<- read.table(file = file_path, head= TRUE, stringsAsFactors = FALSE, sep = " ", comment.char ="#" )
dim(df)
ggplot(df, mapping= aes(y = univ_GPA, x = high_GPA)) + geom_point() + geom_smooth(method = 'lm', se = FALSE)

#Can you predict University GPA from High school GPA ?  


```


# Some important assumptions about linear regression 

1. Linearity: The relationship between the two variables is linear

2. Homoscedasticity: The variance around the regression line is the same for all values of X

3. No auto-correlation: The errors of prediction are distributed normally. This means that the deviations from the regression line are normally distributed.

4. Normality: requires all variables to be multivariate normal  



